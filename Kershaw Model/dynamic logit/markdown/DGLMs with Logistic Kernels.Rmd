---
title: "Dynamic generalized linear model derivations using Polya-gamma data augmentation"
output: pdf_document
header-includes:
  - \usepackage{setspace}\doublespacing
bibliography: biblio.bib
csl: biometrics.csl
---

```{r, include = F}
packs <- c('bayes')
lapply(packs, require, character.only = T)

knitr::opts_chunk$set()
set.seed(04302019)
```

\setlength\parindent{0pt}
\pagenumbering{gobble}

\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfb}{\mbox{\boldmath $\beta$}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfw}{\mbox{\boldmath $\omega$}}
\newcommand{\bfW}{\mbox{\boldmath $\Omega$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}
\newcommand{\bfmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bfS}{\mbox{\boldmath $\Sigma$}}
\newcommand{\bfm}{\mbox{\boldmath $m$}}
\newcommand{\bfV}{\mbox{\boldmath $V$}}

\normalsize

\section{1 Logistic regression}

Consider the standard logistic regression model. 
\[
\begin{split}
y_i &\sim \text{binomial}(n_i, \pi_i) \\
\pi_i &= \frac{\exp(\bfx_i'\bfb)}{1 + \exp(\bfx_i'\bfb)}
\end{split}
\]

To sample the joint posterior distribution of $\bfb$, we place multivariate normal priors on $\bfb$ and implement the Polya-gamma data augmentation strategy described by [@polson2013], which allows for Gibbs draws. The details are provided below. 

\subsection{1.1 Derivations}

The full conditional posterior distribution of the regression coefficients is proportional to the following:
\begin{equation}
\begin{split}
p(\bfb | \bfy) &\propto p(\bfb)\prod_{i=1}^n p(y_i | \bfb) \\
&\propto p(\bfb)\prod_{i=1}^n \pi_i^{y_i} (1 - \pi_i)^{n_i - y_i}, \hspace{5mm} \pi_i = \frac{\exp(\bfx_i'\bfb)}{1 + \exp(\bfx_i'\bfb)}
\end{split}
\end{equation}
This can be rewritten as the following:
\begin{equation}
\begin{split}
p(\bfb | \bfy) &\propto p(\bfb)\prod_{i=1}^n \left(\frac{\exp(\bfx_i'\bfb)}{1 + \exp(\bfx_i'\bfb)}\right)^{y_i} \left(1 - \frac{\exp(\bfx_i'\bfb)}{1 + \exp(\bfx_i'\bfb)}\right)^{n_i - y_i} \\
&= p(\bfb)\prod_{i=1}^n \frac{\exp(\bfx_i'\bfb)^{y_i}}{(1 + \exp(\bfx_i'\bfb))^{n_i}}
\end{split}
\end{equation}
Theorem one of [@polson2013] states that for $b > 0$,
\[
\frac{(e^\psi)^a}{(1 + e^\psi)^b} = 2^{-b}e^{\kappa\psi}\int_0^\infty e^{-\omega\psi^2/2} p(\omega)d\omega,
\]
for $\kappa = a - b/2$ and $\omega \sim \text{PG}(b, 0)$, where PG denotes the Polya-gamma density. Therefore, revisiting (2), conditioning on the Polya-gamma latents, and letting $\psi_i = \bfx_i'\bfb$ we have:
\begin{equation}
\begin{split}
p(\bfb | \bfy, \bfw) &\propto p(\bfb)\prod_{i=1}^n \frac{(e^{\psi_i})^{y_i}}{(1 + e^{\psi_i})^{n_i}} \\
&= p(\bfb)\prod_{i=1}^n \exp(\kappa_i\psi_i - \omega_i\psi_i^2/2) \\
&\propto p(\bfb)\prod_{i=1}^n \exp\left(- \frac{\omega_i}{2} \left(z_i - \psi_i\right)^2\right) \\
&= p(\bfb) \exp\left\{ -\frac{1}{2}(\bfz - \bfX\bfb)' \bfW (\bfz - \bfX\bfb)\right\},
\end{split}
\end{equation}
where $\kappa_i = y_i - \frac{n_i}{2}$, $z_i = \frac{\kappa_i}{\omega_i}$, and $\bfW = \text{diag}(\omega_1, ..., \omega_n)$. From (3), $\bfz$ is conditionally Gaussian. That is,
\begin{equation}
\bfz \sim \mathcal{N}(\bfX\bfb, \bfW^{-1})
\end{equation}
Therefore, placing a $\mathcal{N}(\bfmu_0, \bfS_0)$ prior on $\bfb$ results in the following full conditional distribution:
\begin{equation}
\begin{split}
p(\bfb | \bfz, \bfW) &\propto p(\bfz | \bfb, \bfW) \cdot p(\bfb) \\
&\propto \exp\left\{-\frac{1}{2}\left(\bfz - \bfX\bfb \right)' \bfW \left(\bfz - \bfX\bfb \right) \right\} \exp\left\{-\frac{1}{2}\left(\bfb - \bfmu_0 \right)' \bfS_0^{-1} \left(\bfb - \bfmu_0 \right) \right\} \\
&= \text{exp}\left\{-\frac{1}{2}\left(\bfz'\bfW\bfz - 2\bfb'\bfX'\bfW\bfz + \bfb'\bfX'\bfW\bfX\bfb \right) \right\} \text{exp}\left\{-\frac{1}{2}\left(\bfb'\bfS_0^{-1}\bfb - 2\bfb'\bfS_0^{-1}\bfmu_0 + \bfmu_0'\bfS_0^{-1}\bfmu_0 \right) \right\} \\
&\propto \text{exp}\left\{-\frac{1}{2}\left(- 2\bfb'\bfX'\bfW\bfz + \bfb'\bfX'\bfW\bfX\bfb \right) \right\} \text{exp}\left\{-\frac{1}{2}\left(\bfb'\bfS_0^{-1}\bfb - 2\bfb'\bfS_0^{-1}\bfmu_0 \right) \right\} \\
&= \text{exp}\left\{-\frac{1}{2}\left(- 2\bfb'\bfX'\bfW\bfz + \bfb'\bfX'\bfW\bfX\bfb + \bfb'\bfS_0^{-1}\bfb - 2\bfb'\bfS_0^{-1}\bfmu_0 \right) \right\} \\
&= \text{exp}\left\{-\frac{1}{2}\left(- 2\bfb'\left(\bfX'\bfW\bfz + \bfS_0^{-1}\bfmu_0 \right) + \bfb'\left(\bfX'\bfW\bfX + \bfS_0^{-1}\right) \bfb\right) \right\} \\
\end{split}
\end{equation}
And now, a quick note on identifying kernels of a multivariate normal distribution. Suppose $\bfb \sim \mathcal{N}(\bfmu, \bfS)$. Then
\begin{equation}
\begin{split}
p(\bfb) &\propto \text{exp}\left\{-\frac{1}{2}(\bfb - \bfmu)'\bfS^{-1}(\bfb - \bfmu) \right\} \\
&\propto \text{exp}\left\{-\frac{1}{2}\left(\bfb'\bfS^{-1}\bfb - 2\bfb'\bfS^{-1}\bfmu \right) \right\}
\end{split}
\end{equation}
Therefore, based on (6), (5) implies that 
\begin{equation}
\bfb | \bfz, \bfW \sim \mathcal{N}(\bfm, \bfV),
\end{equation}
where $\bfV = \left(\bfX'\bfW\bfX + \bfS_0^{-1}\right)^{-1}$ and $\bfm = \bfV \left(\bfX'\bfW\bfz + \bfS_0^{-1}\bfmu_0 \right)$. Finally, we note that $\bfW\bfz = \boldsymbol{\kappa}$. 

Finally, [@polson2013] note that the full conditional distribution of $\bfW$ is also in the Polya-gamma family, and given by the following:
\begin{equation}
\omega_i | \bfb \sim \text{PG}(n_i, \psi_i)
\end{equation}
where $\psi_i = \bfx_i'\bfb$. We omit the derivation here. 

\subsection{1.2 Implementation}

As a motivating example, consider the famed Donner party dataset. The MLEs for an additive model with sex and age are given below. 
\singlespacing
```{r}
data(case2001, package = 'Sleuth3')
summary(glm(Status ~ Sex + Age, family = 'binomial', data = case2001))
```

We now fit this model with a Gibbs sampler using the strategy described above. 

```{r, warning = F, message = F, cache = T, eval = T}
# response, size, and design
y <- with(Sleuth3::case2001, as.numeric(Status) - 1) # died = 0, survived = 1
size <- rep(1, nrow(Sleuth3::case2001))
n <- nrow(Sleuth3::case2001)
X <- model.matrix(~ Sex + Age, data = Sleuth3::case2001)

# precompute kappa
kappa <- y - size/2 

# setup sampler and priors
num.mcmc <- 10000
p <- ncol(X)
beta.mcmc <- matrix(0, num.mcmc, p);colnames(beta.mcmc) <- colnames(X)

mu0 <- matrix(0, nrow = p, ncol = 1)
Sigma0.inv <- solve(16*diag(p))
prior.prod <- Sigma0.inv %*% mu0

# initialize
beta <- matrix(rnorm(p), ncol = 1)

# sampler
for(i in 2:num.mcmc){
  # update latent omegas
  eta <- c(X %*% beta)
  omega <- BayesLogit::rpg(n, size, eta)
  Omega <- diag(omega)
  
  # update beta
  V <- solve(t(X) %*% Omega %*% X + Sigma0.inv)
  m <- V %*% (t(X) %*% kappa + prior.prod)
  beta <- matrix(mvtnorm::rmvnorm(1, m, V), ncol = 1)
  
  # store
  beta.mcmc[i, ] <- c(beta)
}
est <- cbind(
  colMeans(beta.mcmc),
  apply(beta.mcmc, 2, sd)
); colnames(est) <- c('mean', 'sd')
est
```

These estimates and standard deviations are consistent with those of the MLEs, which reflects our weakly informative priors. 

\newpage

\section{2 Dynamic logistic regression}

We now allow the regression coefficients to change over time. 
\[
\begin{split}
y_t &\sim \text{binomial}(n_t, \pi_t), \hspace{5mm} \pi_t = \frac{\exp(\bfx_t'\bfb_t)}{1 + \exp(\bfx_t'\bfb_t)} \\
\bfb_t &= \bfb_{t-1} + \bfV_t, \hspace{12mm} \bfV_t \sim \mathcal{N}(\boldsymbol{0}, \tau^2\boldsymbol{I})
\end{split}
\]


\newpage
\singlespacing
\section{References}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}

\noindent \bibliography{biblio}
